#!/bin/bash

#allow passing some environment vairables to override some automated steps
if [ -z "$DATADIR" ]; then
    DATADIR=""
fi

if [ -z "$ORIGIN_URL" ]; then
    ORIGIN_URL=""
fi

if [ -z "$DEFAULT_BRANCH" ]; then
    DEFAULT_BRANCH=""
fi

if [ -z "$FAIL_ON_RENAME" ]; then
    FAIL_ON_RENAME=false
fi

set -eu -o pipefail

# cross-OS compatibility (greadlink, gsed, gzcat are GNU implementations for OS X)
[[ $(uname) == 'Darwin' ]] && {
    shopt -s expand_aliases
    which greadlink gsed gzcat gjoin gmktemp gdate gwc > /dev/null && {
        unalias readlink sed zcat join mktemp date wc >/dev/null 2>/dev/null
        alias readlink=greadlink sed=gsed zcat=gzcat join=gjoin mktemp=gmktemp date=gdate wc=gwc
    } || {
        echo 'ERROR: GNU utils required for Mac. You may use homebrew to install them: brew install coreutils gnu-sed'
        exit 1
    }
}

# The sample sizes don't represent how many PRs we look at.
# There's a github API that lists all of the PRs associated with a commit,
# but we would have to call that on a per commit basis which is too time consuming to be practical.
# Therefore, we apply a heuristic to get those associations, but we don't know how good that heuristic is.
# So we call the aforementioned Github API on a sample
# and compare that to our heuristic to determine how good our heuristic is.
PR_SAMPLESIZE=${PR_SAMPLESIZE:-250}
COMMIT_PR_SAMPLESIZE=${COMMIT_PR_SAMPLESIZE:-200}

#an outer bound for how many pages of PRs we'll try to fetch
#the vast majority (>95%) of OSS benchmark repos are captured by just 20 pages
MAX_ALL_PR_PAGES=600
#an outer bound for how much data we'll pull from github, 20 years
MAX_SPAN_DAYS=${MAX_SPAN_DAYS:-7300}
#we'll run sets of time-bound analyses:
SPAN_DAYS="183 365 730 1460"

DIR="$( cd "$( dirname "${BASH_SOURCE[0]}" )" >/dev/null 2>&1 && pwd )"

echo "this tool requires several other tools to be installed."
echo "if they are not the dependency check that follows will fail."
echo "see README.md in the same folder as statstool for details."
echo
echo "checking for dependencies..."
echo "checking for bash..."
bash --version 2>&1 | grep 'bash'
echo "checking for gzip..."
gzip --version 2>&1 | grep 'gzip'
echo "checking for zcat..."
zcat --version 2>&1 | grep 'gzip'
echo "checking for curl..."
curl --version | grep 'curl 7\.[0-9]'
echo "checking for git..."
git --version | grep 'git version [0-9]'
echo "checking for sed..."
sed --version | grep "sed.* 4\."
echo "checking for awk..."
gawk --version | grep 'GNU Awk [0-9]'
echo "checking for join..."
join --version | grep 'join (GNU coreutils) [0-9]'
echo "checking for pv..."
pv --version | grep 'pv [0-9]'
echo "checking for ag..."
ag --version | grep 'ag version [0-9]'
echo "checking for mktemp..."
mktemp --version | grep 'mktemp'
echo "checking for date..."
date --version | grep 'GNU coreutils'
echo "checking for wc..."
wc --version | grep 'GNU coreutils'

echo "running a quick test..."
testfile=$(mktemp tmp.XXXXXXXXXX.gz)
echo "hello" | gzip -c > $testfile
zcat $testfile | sed 's/hello/hiya/' | gawk '/hiya/ { printf("{\"message\":\"sed gawk pv jq and ag all work\"}\n")}' | pv -s 1 -l | jq '.message' | ag 'sed gawk pv jq and ag all work'
zcat $testfile | wc -l | ag '^1$'
rm $testfile

test_date="$(date --utc)"
date -d@$(date -d "${test_date}" +%s) --utc | grep "${test_date}"

echo "dependencies look good"
echo

echo "we need to gather some data before we can start computing some stats"
echo "first we'll process some local commit data using things like git log and git show"
echo "second we'll download some data from the GitHub like pulls, issues and comments"
echo "we are only going to read data from your repo and GitHub"
echo "all of this can take 15-30 minutes, so sit back and relax"
echo

#get to toplevel of the repo which some steps below assume we are at
cd $(git rev-parse --show-toplevel)

ORIGIN_REMOTE="origin"

if [ -z "$ORIGIN_URL" ]; then
    #try to autodetect the github repo
    echo "autodetecting the origin remote url"
    echo

    ORIGIN_URL="$(git config --get-regex remote.${ORIGIN_REMOTE}.url | gawk '{print $2}')" || true

    if [ -z "$ORIGIN_URL" ] || ! echo "$ORIGIN_URL" | grep -E '^(git@|https:\/\/)([^:@/]*(:[^:@]*)?@)?github.com[:\/]([^\/]*\/[^\/]*)(\.git)?$' > /dev/null; then
        echo "it looks like you don't have an origin remote, or it's not pointing to a github repository."
        echo "you can provide your own github repository, check out the README"
        exit 1
    fi
else
    #let the user provide their own origin url
    echo "using provided origin url ${ORIGIN_URL}"
    echo

    if ! echo "$ORIGIN_URL" | grep -E '^(git@|https:\/\/)github.com[:\/]([^\/]*\/[^\/]*)(\.git)?$' > /dev/null; then
        echo "it looks like the origin url you provided (${ORIGIN_URL}) isn't a github repository url."
        echo "check the README.md or contact AutoDev folks for more assistance."
        exit 1
    fi

fi

REPO=$(echo "$ORIGIN_URL" | sed 's/\.git$//' | sed -E '/^(git@|https:\/\/([^:@/]*(:[^:@]*)?@)?)github.com[:\/]/!{q1}; {s/.*github.com[:\/]([^\/]*\/[^\]*)$/\1/}')

echo "using remote url ${ORIGIN_URL}"
echo "this corresponds to the github repository at https://github.com/${REPO}"
echo "this is the repository whose pull requests and issues we'll analyze"
echo

if [ ! -z "${DATADIR}" ]; then
    echo "using given path to hold temp data"
    mkdir "${DATADIR}"
fi
echo "we'll write data to ${DATADIR}"
echo

#truncate timing data
> ${DATADIR}/times.tsv

#record when we start pulling data
#this can be used as a good timestamp for when data collection happened
printf "data_pull_start_time\t%d\n" $data_pull_start_time >> ${DATADIR}/times.tsv

echo "checking access to github for ${REPO}..."
REPO_URL="https://api.github.com/repos/${REPO}"
echo ${REPO_URL} | GITHUB_TOKEN=$GITHUB_TOKEN ${DIR}/fetch-comments.sh > ${DATADIR}/repo
if ! cat ${DATADIR}/repo | jq '.full_name' | ag -v '^null$' > /dev/null; then
    echo "no access to ${REPO_URL}"
    echo "check that your access token has access to the repos scope"
    echo "see GitHub's documentation here: https://help.github.com/en/github/authenticating-to-github/creating-a-personal-access-token-for-the-command-line"
    echo "we need the \"repo\" top-level scope as described in the tutorial"
    exit 1
elif ! cat ${DATADIR}/repo | jq '.full_name' | ag '^"'"${REPO}"'"$' > /dev/null; then
    NEW_REPO_NAME=$(cat ${DATADIR}/repo | jq '.full_name')
    echo "it appears that ${REPO} has been renamed to ${NEW_REPO_NAME}"
    if $FAIL_ON_RENAME; then
        echo "exiting because of repo rename"
        exit 1
    fi
fi
echo "access to github looks good."
echo

#figure out the default branch
if [ -z "$DEFAULT_BRANCH" ]; then
    echo "autodtecting default branch from github configuration..."
    echo

    DEFAULT_BRANCH=$(cat ${DATADIR}/repo | jq -r '.default_branch')

    #TODO: check that default branch is tracked by $REPO
    implicated_remote=$(git config --get-regex branch.${DEFAULT_BRANCH}.remote | gawk '{print $2}') || true
    if ! git config --get-regex remote.${implicated_remote}.url | grep $REPO > /dev/null; then
        echo "it appears that ${DEFAULT_BRANCH} is not tracked by the github repository ${REPO}"
        echo "if you really want to proceed with analysis, you can override both ORIGIN_URL and DEFAULT_BRANCH and we won't check for this. see the README for more information"
        echo
        exit 1
    fi
else
    echo "using provided default branch ${DEFAULT_BRANCH}"
    echo
fi

echo "default branch is ${DEFAULT_BRANCH}"
echo "this is the branch whose history we'll process for merges and pulls"
echo

if ! git branch | grep "$DEFAULT_BRANCH" > /dev/null; then
    echo "you don't appear to have a branch called ${DEFAULT_BRANCH}"
    echo "we can't proceed without the default branch checked out"
    echo "perhaps you need to run git fetch ${ORIGIN_REMOTE} ${DEFAULT_BRANCH}?"
    exit 1
fi

echo "preparing code history..."
starttime=$(date +%s)
DATADIR=${DATADIR} DEFAULT_BRANCH=${DEFAULT_BRANCH} FILE_EXCLUDE_PATHS=${DIR}/excludefiles.regex ${DIR}/build_features.sh
code_history_time=$(( $(date +%s) - ${starttime}))
echo "done preparing code history in ${code_history_time}s."
echo
printf "code_history_time\t%f\n" ${code_history_time} >> ${DATADIR}/times.tsv

echo "preparing commit messages..."
git log "${DEFAULT_BRANCH}" --first-parent --format='__commit__ %H%x0A%B' -- > ${DATADIR}/commit_messages
git log "${DEFAULT_BRANCH}" --format='%H%x09%ae' -- | LC_ALL=C sort > ${DATADIR}/commits_with_author
git log "${DEFAULT_BRANCH}" --topo-order --format='%H%x09%P%x09%ce%x09%ct%x09%ae%x09%at' -- | gzip -c > ${DATADIR}/commit_graph.gz
echo "done."
